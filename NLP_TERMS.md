# üß† O que √© NLP (Processamento de Linguagem Natural)?

NLP (Natural Language Processing) √© uma √°rea da Intelig√™ncia Artificial que tem como objetivo permitir que computadores **compreendam, interpretem, manipulem e gerem** a linguagem humana ‚Äî seja escrita ou falada ‚Äî de maneira √∫til e eficiente. Para isso, o NLP combina conhecimentos de lingu√≠stica, estat√≠stica e t√©cnicas de machine learning.

---

# üß© Como funciona o NLP?

O processamento de linguagem natural envolve v√°rias etapas que transformam textos ou falas em dados estruturados que m√°quinas podem entender. Veja as principais fases:

## üî† 1. Tokeniza√ß√£o

√â o processo que **divide o texto em partes menores**, geralmente palavras ou frases, chamadas de *tokens*. Essa etapa √© fundamental para que o texto possa ser analisado em unidades menores.

Exemplo:

```python
"Eu gosto de Python"  ‚ûú  ["Eu", "gosto", "de", "Python"]
```

---

## üõë 2. Remo√ß√£o de Stopwords

Stopwords s√£o palavras muito comuns e que, na maioria dos casos, n√£o agregam muito significado para a an√°lise, como artigos, preposi√ß√µes e conjun√ß√µes ("de", "o", "e", "a"). Remov√™-las ajuda a focar nas palavras mais importantes do texto.

---

## üîÅ 3. Stemming e Lemmatization

Ambos s√£o t√©cnicas para **reduzir palavras √†s suas formas b√°sicas ou ra√≠zes**, para agrupar varia√ß√µes da mesma palavra.

* **Stemming:** T√©cnica mais simples e r√°pida que remove sufixos para chegar a um radical aproximado. Pode gerar palavras truncadas e n√£o muito "corretas".
  Exemplo: "gostando", "gostei" ‚ûú "gost"

* **Lemmatization:** T√©cnica mais avan√ßada que utiliza dicion√°rios e regras gramaticais para encontrar a forma base correta da palavra (o *lem*).
  Exemplo: "gostando", "gostei" ‚ûú "gostar"

---

## üßÆ 4. Vetoriza√ß√£o (Representa√ß√£o num√©rica do texto)

Modelos de machine learning precisam de n√∫meros, ent√£o o texto deve ser convertido em vetores num√©ricos que representem seu conte√∫do.

Principais m√©todos:

* **Bag of Words (BoW):** Conta a frequ√™ncia de cada palavra no texto, sem considerar a ordem ou contexto.

* **TF-IDF (Term Frequency-Inverse Document Frequency):** Calcula a import√¢ncia de uma palavra no documento e em todo o conjunto de textos, dando peso maior para palavras relevantes e menos frequentes.

* **Word Embeddings:** Representa√ß√µes vetoriais densas que capturam o contexto e significado das palavras, como Word2Vec, GloVe ou modelos mais avan√ßados como BERT.

---

## ü§ñ 5. Treinamento com Machine Learning

Depois de transformar os textos em n√∫meros, algoritmos de machine learning podem aprender padr√µes e fazer previs√µes ou classifica√ß√µes.

Alguns modelos populares para NLP:

**Regress√£o Log√≠stica:** Modelo simples e eficiente para classifica√ß√£o bin√°ria ou multiclasse.

**Naive Bayes:** Baseado em probabilidade, funciona bem para textos e √© r√°pido.

**SVM (Support Vector Machines):** Modelo poderoso para classifica√ß√£o com alta dimensionalidade.

**√Årvores de Decis√£o:** Modelos interpret√°veis que dividem os dados em regras.

**Redes Neurais:** Desde simples perceptrons at√© redes profundas, s√£o a base de muitos avan√ßos recentes.

**Transformers:** Arquiteturas modernas (como BERT, RoBERTa, GPT) que capturam contexto complexo e s√£o usadas em tarefas avan√ßadas.

---

# üîç Interpretando Modelos com LIME e SHAP

Mesmo que um modelo apresente boa performance, √© essencial entender **como e por que ele toma certas decis√µes** para garantir confian√ßa, detectar erros e melhorar o sistema.

## üü° LIME (Local Interpretable Model-Agnostic Explanations)

LIME √© uma t√©cnica que explica **previs√µes individuais** de qualquer modelo, mesmo os complexos, de forma simples e local.

Como funciona:

* Cria pequenas perturba√ß√µes na entrada (ex: altera palavras do texto).

* Observa como a sa√≠da do modelo muda com essas perturba√ß√µes.

* Ajusta um modelo simples (ex: regress√£o linear) que aproxima o comportamento do modelo original naquele ponto.

**No NLP**, LIME destaca as palavras que mais influenciaram a decis√£o para aquela inst√¢ncia espec√≠fica.

Exemplo visual:

Texto: "O produto √© incr√≠vel, eu amei"
‚Üí LIME destaca "incr√≠vel" e "amei" como palavras que contribu√≠ram para classificar como "positivo".

---

## üî¥ SHAP (SHapley Additive exPlanations)

SHAP √© uma t√©cnica baseada em teoria dos jogos que calcula a contribui√ß√£o individual de cada caracter√≠stica (palavra, no caso de NLP) para a previs√£o do modelo.

Caracter√≠sticas do SHAP:

* Funciona para qualquer modelo.

* Oferece explica√ß√µes **globais** (entender o modelo como um todo) e **locais** (explicar uma previs√£o espec√≠fica).

* Garante propriedades matem√°ticas importantes, como consist√™ncia e justi√ßa na atribui√ß√£o de import√¢ncia.

**No NLP**, SHAP mostra se cada palavra **aumenta ou diminui** a probabilidade de uma determinada classe, ajudando a identificar vi√©s ou problemas.

Exemplo visual:

Texto: "N√£o gostei do atendimento"
‚Üí SHAP mostra que a palavra "n√£o" reduz fortemente a chance da classifica√ß√£o ser positiva.

---

## ‚úÖ Por que usar LIME ou SHAP?

**Confian√ßa e transpar√™ncia:** Entender *por que* o modelo tomou determinada decis√£o.

**Detec√ß√£o de erros e vi√©s:** Identificar falhas ou preconceitos presentes nos dados ou no modelo.

**Melhoria do modelo:** Ajustar ou refinar o sistema com base no impacto das palavras/caracter√≠sticas.
